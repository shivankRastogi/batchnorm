{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "577c5e46-32b4-42a9-ad8b-a04747bde633",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt # for making figures\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2da8277f-660e-4ddb-9063-4d917d311995",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in all the words\n",
    "words = open('names.txt', 'r').read().splitlines()\n",
    "words[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7bc419b-41d3-4807-a244-70d484cf9c7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32033"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9d0141e-92aa-472e-b5a8-6426522483a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(''.join(words))))\n",
    "charIntIndexMapping = {s:i+1 for i,s in enumerate(chars)}\n",
    "charIntIndexMapping['.'] = 0\n",
    "indexToCharMapping ={i:s for s,i in charIntIndexMapping.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9aef2b85-ab06-4e44-a4cc-63bd54d81ab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182625, 3]) torch.Size([182625])\n",
      "torch.Size([22655, 3]) torch.Size([22655])\n",
      "torch.Size([22866, 3]) torch.Size([22866])\n"
     ]
    }
   ],
   "source": [
    "#1. Build the dataset: Train, Validation and Test\n",
    "block_size = 3 \n",
    "\n",
    "def build_dataset(words):  \n",
    "\n",
    "    block_size = 3 \n",
    "    inputContext, outputCharcterForThatContext = [], []\n",
    "    \n",
    "    for w in words:\n",
    "      \n",
    "    \n",
    "      contextSlidingWindow = [0] * block_size\n",
    "        \n",
    "      for ch in w + '.': \n",
    "        index = charIntIndexMapping[ch]\n",
    "        inputContext.append(contextSlidingWindow)\n",
    "        outputCharcterForThatContext.append(index)\n",
    "    #print(''.join(indexToCharMapping[i] for i in context), '--->', indexToCharMapping[ix])\n",
    "        contextSlidingWindow = contextSlidingWindow[1:] + [index]\n",
    "\n",
    "    inputContext = torch.tensor(inputContext)\n",
    "    outputCharcterForThatContext = torch.tensor(outputCharcterForThatContext)\n",
    "    print(inputContext.shape,outputCharcterForThatContext.shape)\n",
    "    return inputContext,outputCharcterForThatContext\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "noOfWordForTraining = int(0.8*len(words))\n",
    "noOfWordsForValidation = int(0.9*len(words))\n",
    "\n",
    "inputContextTrain, outputCharacterTrain = build_dataset(words[:noOfWordForTraining])\n",
    "inputContextValidation, outputCharacterValidation = build_dataset(words[noOfWordForTraining:noOfWordsForValidation])\n",
    "inputContextTesting, outputCharacterTesting = build_dataset(words[noOfWordsForValidation:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b8bd249a-bc56-4a23-a0f3-b9a3897d4abe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10281\n"
     ]
    }
   ],
   "source": [
    "#2. MLP revisited: Creating Parameters for MLP(Charembedding, Weights and biases)\n",
    "\n",
    "n_embd = 2 # the dimensionality of the character embedding vectors\n",
    "n_hidden = 300 # the number of neurons in the hidden layer of the MLP(1st layer)\n",
    "vocab_size=27 #unique characters we have including '.'\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "charEmbeddings  = torch.randn((vocab_size, n_embd),generator=g,requires_grad=True)\n",
    "weightLayer1 = torch.randn((n_embd * block_size, n_hidden), generator=g,requires_grad=True)\n",
    "biasLayer1 = torch.randn(n_hidden,generator=g,requires_grad=True)\n",
    "weightLayer2 = torch.randn((n_hidden, vocab_size),generator=g,requires_grad=True)\n",
    "biasLayer2 = torch.randn(vocab_size,generator=g,requires_grad=True)\n",
    "\n",
    "parameters = [charEmbeddings, weightLayer1, weightLayer2, biasLayer1, biasLayer2]\n",
    "print(sum(p.nelement() for p in parameters)) # number of parameters in total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "367082a0-84d3-49a3-9d67-cada6788d43d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0/ 200000: 27.0705\n",
      "  10000/ 200000: 2.8543\n",
      "  20000/ 200000: 2.4815\n",
      "  30000/ 200000: 2.7685\n",
      "  40000/ 200000: 2.1235\n",
      "  50000/ 200000: 2.3941\n",
      "  60000/ 200000: 2.3548\n",
      "  70000/ 200000: 2.4956\n",
      "  80000/ 200000: 2.5888\n",
      "  90000/ 200000: 2.3835\n",
      " 100000/ 200000: 2.7417\n",
      " 110000/ 200000: 2.4000\n",
      " 120000/ 200000: 2.2243\n",
      " 130000/ 200000: 2.4927\n",
      " 140000/ 200000: 2.6062\n",
      " 150000/ 200000: 2.4692\n",
      " 160000/ 200000: 2.5604\n",
      " 170000/ 200000: 2.5747\n",
      " 180000/ 200000: 2.2426\n",
      " 190000/ 200000: 2.5037\n",
      "loss on training data set 2.052839994430542\n"
     ]
    }
   ],
   "source": [
    "#3. Training the model\n",
    "max_steps=200000\n",
    "batch_size=32\n",
    "lossi = []\n",
    "stepsi = []\n",
    "noOfColAfterFlattening = block_size*n_embd\n",
    "for i in range(max_steps):\n",
    "    noOfWindowsToBeUsedPerLearningStep = batch_size\n",
    "    randomSlidingWindowsUsed = torch.randint(0, inputContextTrain.shape[0], (noOfWindowsToBeUsedPerLearningStep,))\n",
    "    inputForLayer1 = charEmbeddings[inputContextTrain[randomSlidingWindowsUsed]]\n",
    "    inputForLayer1 = inputForLayer1.view(noOfWindowsToBeUsedPerLearningStep, noOfColAfterFlattening)\n",
    "    h = torch.tanh(inputForLayer1@weightLayer1+biasLayer1)\n",
    "    logits = h@weightLayer2+biasLayer2\n",
    "    loss = F.cross_entropy(logits,outputCharacterTrain[randomSlidingWindowsUsed])\n",
    "    for p in parameters:\n",
    "        p.grad = None \n",
    "    loss.backward()\n",
    "    for p in parameters:\n",
    "        p.data += -0.1*p.grad \n",
    "    #track stats    \n",
    "    if i % 10000 == 0: # print every once in a while\n",
    "        print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}') #logging the loss in every 10,000 learning steps \n",
    "    lossi.append(loss.item())\n",
    "    stepsi.append(i)\n",
    "\n",
    "   \n",
    "\n",
    "print(\"loss on training data set\",loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75c27b6-6eb1-477e-b87a-1cefd1f3b049",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(stepsi,lossi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c07e820-395f-40c3-8352-c26573e515bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainingModel(inputContextTrain,charEmbeddings,weightLayer1,biasLayer1,weightLayer2,biasLayer2):\n",
    "    max_steps=200000\n",
    "    batch_size=32\n",
    "    lossi = []\n",
    "    stepsi = []\n",
    "    parameters = [charEmbeddings, weightLayer1, weightLayer2, biasLayer1, biasLayer2]\n",
    "    noOfColAfterFlattening = block_size*n_embd\n",
    "    for i in range(max_steps):\n",
    "        noOfWindowsToBeUsedPerLearningStep = batch_size\n",
    "        randomSlidingWindowsUsed = torch.randint(0, inputContextTrain.shape[0], (noOfWindowsToBeUsedPerLearningStep,))\n",
    "        inputForLayer1 = charEmbeddings[inputContextTrain[randomSlidingWindowsUsed]]\n",
    "        inputForLayer1 = inputForLayer1.view(noOfWindowsToBeUsedPerLearningStep, noOfColAfterFlattening)\n",
    "        h = torch.tanh(inputForLayer1@weightLayer1+biasLayer1)\n",
    "        logits = h@weightLayer2+biasLayer2\n",
    "        loss = F.cross_entropy(logits,outputCharacterTrain[randomSlidingWindowsUsed])\n",
    "        for p in parameters:\n",
    "            p.grad = None \n",
    "        loss.backward()\n",
    "        lr = 0.1 if i<100000 else 0.01\n",
    "        for p in parameters:\n",
    "            p.data += -lr * p.grad \n",
    "        #track stats    \n",
    "        if i % 10000 == 0: # print every once in a while\n",
    "            print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}') #logging the loss in every 10,000 learning steps \n",
    "        lossi.append(loss.item())\n",
    "        stepsi.append(i)\n",
    "    \n",
    "       \n",
    "    plt.plot(stepsi,lossi)\n",
    "    print(\"loss on training data set\",loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e03020a-4c9b-4db9-8b34-9129a2a0cfbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4 PROBLEM-1 WITH OUR MODEL(FIXING THE INITIAL LOSS)\n",
    "\n",
    "#1.LOSS BEFORE FIRST LEARNING STEP IS TOO HIGH\n",
    "    #a. For the first learning step the loss is too high i.e 27.49, as seen from graph in next leanring steps it goes down drastically which\n",
    "    #suggests that something is wrong with our parameters with which we start with\n",
    "\n",
    "    #b. so we have 27 character possible for any place and in starting when our model is not trained every character should \n",
    "    #be equally likely so prob for any character to come at a place is 1/27 and we do negative log likelihood which is 3.29\n",
    "    # and since loss is basically mean so our loss to start with is 3.29 and not 27.49 this suggests something is wrong\n",
    "    #with our model to params with which we start with\n",
    "\n",
    "#2. SOLUTION:(LOGITS is before counts step i.e the step where we do exponentiation for representing count)\n",
    "    #a. We want for the first learning step logits to have value closer to each other the more wide i.e different the logit value are\n",
    "    #the less equilikely will be prob of occurences and hence the more will be loss (i.e more bigger than 3.29 loss which is for equilikely)\n",
    "    #b. We want logits[i][j] to have equal like values, so we want logits[i][j] to have value closer to 0\n",
    "    #c. We make b2(bias for layer-2) to be 0 start with Weights for layer to be multiplied by 0.1\n",
    "    #d. We dont want to multiply weights with 0, multiplying bias with 0 is fine\n",
    "\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "charEmbeddings  = torch.randn((vocab_size, n_embd),generator=g,requires_grad=True)\n",
    "weightLayer1 = torch.randn((n_embd * block_size, n_hidden), generator=g,requires_grad=True)\n",
    "biasLayer1 = torch.randn(n_hidden,generator=g,requires_grad=True)\n",
    "weightLayer2 = torch.randn((n_hidden, vocab_size),generator=g)*0.01\n",
    "biasLayer2 = torch.randn(vocab_size,generator=g)*0\n",
    "\n",
    "weightLayer2.requires_grad=True\n",
    "biasLayer2.requires_grad=True\n",
    "\n",
    "#weightLayer2 = torch.randn((n_hidden, vocab_size),generator=g, required_grad=True)*0.1 was causing error when we were multiplyinh\n",
    "#learning rate with gradient, Error was cant multiply none with fload hence separated out requires_grad=true in separate line\n",
    "\n",
    "parameters = [charEmbeddings, weightLayer1, weightLayer2, biasLayer1, biasLayer2]\n",
    "trainingModel(inputContextTrain,charEmbeddings,weightLayer1,biasLayer1,weightLayer2,biasLayer2)\n",
    "\n",
    "\n",
    "#3. Observations from Solution:\n",
    "    #1. Loss for first step drastically reduces from very big value like 27 to 3.25 which we want, because equilikely prob=3.29\n",
    "    #2. The plot of loss is no more hockey shape, earlier we had hockey stick like shape where we start with very high loss and\n",
    "    # immediately go to lower loss, now we have fixed that issue so we have removed easy gains from our model \n",
    "    #3. Since we start with better model, we dont have to waste initial learning steps to get that easy gain which results in lesser loss\n",
    "    # in model of 1.91 incomparison to previous training where we have not multiplied with 0.01 and 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ccfd68-08e9-4a02-a21b-37abcef224dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x = np.linspace(-10, 10, 400)\n",
    "y = np.tanh(x)\n",
    "plt.figure(figsize=(4, 3))\n",
    "plt.plot(x, y, label='tanh(x)', color='blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2fe7b67-ced9-46ff-baa5-5f264dc833e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#5. PROBLEM-2 WITH OUR MODEL(FIXING SATURATED tanh)\n",
    "    #1. We do  h = torch.tanh(inputForLayer1@weightLayer1+biasLayer1), we squash the result of layer-1 to a range of -1 to 1 \n",
    "    # from the plot of tanh we see for value big enough like more than 3 or less than -3 tanh() gives value equal to +1 and -1 \n",
    "    #respectively\n",
    "    \n",
    "    #2. Gradient for these portions in tanh graph is zero because graph is flat for x>+3 and x<-3\n",
    "    \n",
    "    #3.When we do gradient decent in backward pass the impact of those cells of matrix resulting from \n",
    "    #h = torch.tanh(inputForLayer1@weightLayer1+biasLayer1) whose value is big enough will not be optimised because their \n",
    "    #gradient is zero, Suppose if entire column of matrix h is 1 or -1 then that neuron's weight can never be optimised and\n",
    "    #it will be a dead neuron\n",
    "    \n",
    "    #4 Sometimes having very large learning rate also results in dead neuron problem\n",
    "\n",
    "#SOLUTION: \n",
    "#1. Mutiply bias and weight for the layer where we are using tanh as activation with very small number like 0.01 so\n",
    "# value from matrix multiplication is very small\n",
    "#2. We multiply bias of layer-1 with 0.01 and weight of layer-1 with 0.2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb500e7-167b-466f-939a-85e027f3a78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#5.2 Continued: PRINTING H matrix BEFORE OPTIMIZATION\n",
    "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "charEmbeddings  = torch.randn((vocab_size, n_embd),generator=g,requires_grad=True)\n",
    "weightLayer1 = torch.randn((n_embd * block_size, n_hidden), generator=g) \n",
    "biasLayer1 = torch.randn(n_hidden,generator=g) \n",
    "\n",
    "\n",
    "#Printing the  tanh matrix we get from first layer output from first learning step\n",
    "noOfWindowsToBeUsedPerLearningStep = batch_size\n",
    "randomSlidingWindowsUsed = torch.randint(0, inputContextTrain.shape[0], (noOfWindowsToBeUsedPerLearningStep,))\n",
    "inputForLayer1 = charEmbeddings[inputContextTrain[randomSlidingWindowsUsed]]\n",
    "inputForLayer1 = inputForLayer1.view(noOfWindowsToBeUsedPerLearningStep, noOfColAfterFlattening)\n",
    "h = torch.tanh(inputForLayer1@weightLayer1+biasLayer1)\n",
    "\n",
    "plt.hist(h.view(-1).tolist(),50) #50 represents number of intervals we want to distribute data into\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.imshow(h.abs()>0.99,cmap='gray',interpolation='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e97b9eb-eb51-41ba-951b-fcec72b96765",
   "metadata": {},
   "outputs": [],
   "source": [
    "#5.3 Continued: PRINTING H matrix AFTER OPTIMIZATION\n",
    "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "charEmbeddings  = torch.randn((vocab_size, n_embd),generator=g,requires_grad=True)\n",
    "weightLayer1 = torch.randn((n_embd * block_size, n_hidden), generator=g)*0.01 \n",
    "biasLayer1 = torch.randn(n_hidden,generator=g)*0.01 \n",
    "\n",
    "\n",
    "#Printing the  tanh matrix we get from first layer output from first learning step\n",
    "noOfWindowsToBeUsedPerLearningStep = batch_size\n",
    "randomSlidingWindowsUsed = torch.randint(0, inputContextTrain.shape[0], (noOfWindowsToBeUsedPerLearningStep,))\n",
    "inputForLayer1 = charEmbeddings[inputContextTrain[randomSlidingWindowsUsed]]\n",
    "inputForLayer1 = inputForLayer1.view(noOfWindowsToBeUsedPerLearningStep, noOfColAfterFlattening)\n",
    "h = torch.tanh(inputForLayer1@weightLayer1+biasLayer1)\n",
    "\n",
    "plt.hist(h.view(-1).tolist(),50)# view(-1) flattens matrix to 1-d array\n",
    "\n",
    "#OBSERVATIONS: \n",
    "#From PLOT-1(Blue histogram):\n",
    "    #We have quite brough value of tanh in range less than -1 and 1, no cell in matrix have value =1 or -1\n",
    "    #this results in no cell have 0 gradient\n",
    "\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.imshow(h.abs()>0.99,cmap='gray',interpolation='nearest')\n",
    "\n",
    "#OBSERVATIONS-2:\n",
    "#FROM PLOT-2(Bar code like matrix)\n",
    "    #1. Barcode is matrix we get from layer-1 after tanh activation function, cell have gray color if h[i][j]>abs(0.99)\n",
    "    #2. So all the cells which have which have white/gray color in the matrix have absolute value>0.99 which means they \n",
    "        #can contribute to zero gradient\n",
    "    #3. If full column is white then that means we have that neuron (represented by that col) as dead neuron\n",
    "    #We can see without optimization as well we didnot have problem of dead neuron atleast!!!!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed21ca49-a77b-4c08-b4ce-4a07f63428ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#5.4 Continued Training the model on train set(after solving problem-2)\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "charEmbeddings  = torch.randn((vocab_size, n_embd),generator=g,requires_grad=True)\n",
    "weightLayer1 = torch.randn((n_embd * block_size, n_hidden), generator=g)*0.2\n",
    "biasLayer1 = torch.randn(n_hidden,generator=g)*0.01\n",
    "weightLayer2 = torch.randn((n_hidden, vocab_size),generator=g)*0.01\n",
    "biasLayer2 = torch.randn(vocab_size,generator=g)*0\n",
    "\n",
    "weightLayer2.requires_grad=True\n",
    "biasLayer2.requires_grad=True\n",
    "weightLayer1.requires_grad=True\n",
    "biasLayer1.requires_grad=True\n",
    "\n",
    "parameters = [charEmbeddings, weightLayer1, weightLayer2, biasLayer1, biasLayer2]\n",
    "trainingModel(inputContextTrain,charEmbeddings,weightLayer1,biasLayer1,weightLayer2,biasLayer2)\n",
    "\n",
    "#OBSERVATION-2 Not much improvement in result probably because there might be lots of cell in the h matrix( the tanh matrix got\n",
    "#from activation layer after layer-1) with big values causing +1 and -1 but whole column of tanh matrix were not having +1 or -1 value \n",
    "#i.e we didnot have dead neuron problem, this is can be seen from 5.2 observation-2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7f42e9-3da4-4823-ad40-ab4902e7a34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#6. Kaiming init: How to do scientifically initialise the numbers we were multiplying with weights and biases of neural net\n",
    "#(Revise Gaussian Distribution, Standard deviation and variance)\n",
    "\n",
    "#1. Random Example to understand things better\n",
    "x = torch.randn(1000,10) #sample inputContex which is flattened, so we have 1000 context windows and 10 block_size * char embedding dimen\n",
    "w = torch.randn(10,200)# Sample Weight matrix for layer-1 with 10 weights and 200 neurons\n",
    "\n",
    "\n",
    "randomSlidingWindowsUsed = torch.randint(0, inputContextTrain.shape[0], (32,))\n",
    "inputForLayer1 = charEmbeddings[inputContextTrain[randomSlidingWindowsUsed]]\n",
    "y = x@w #Sample matrix multiplication\n",
    "#plt.figure(figsize=(20,5))\n",
    "plt.subplot(311)\n",
    "plt.hist(x.view(-1).tolist(),50, density=True) #view(-1) flattens matrix to 1-d array\n",
    "\n",
    "plt.subplot(312)\n",
    "plt.hist(y.view(-1).tolist(),50, density=True)\n",
    "\n",
    "plt.subplot(313)\n",
    "plt.hist(inputForLayer1.view(-1).tolist(),50, density=True) #view(-1) flattens matrix to 1-d array\n",
    "\n",
    "\n",
    "#2. Understanding X-Axis and Y-Axis for the histogram plot\n",
    "#1. x-axis represents the values in \n",
    "\n",
    "#3. Observation-1 The width of gaussian distribution is more in inputcontext plot than the matrix we get after matrix multiplication\n",
    "\n",
    "#4 Inference: \n",
    "    #1. Plot of input context have more width than plot of y matrix which means that in plot for y more elements have value different \n",
    "    # than the mean value which means that element of y matrix are more different/outlier than mean value of y matrix element\n",
    "    #2.Ideally we dont want distribution of the element relative to mean value to change in input context matrix and \n",
    "    # output from matrix multiplicartion matrix y\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # MLP revisited\n",
    "# n_embd = 10 # the dimensionality of the character embedding vectors\n",
    "# n_hidden = 200 # the number of neurons in the hidden layer of the MLP\n",
    "\n",
    "# g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "# C  = torch.randn((vocab_size, n_embd),            generator=g)\n",
    "# W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/((n_embd * block_size)**0.5) #* 0.2\n",
    "# #b1 = torch.randn(n_hidden,                        generator=g) * 0.01\n",
    "# W2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.01\n",
    "# b2 = torch.randn(vocab_size,                      generator=g) * 0\n",
    "\n",
    "# # BatchNorm parameters\n",
    "# bngain = torch.ones((1, n_hidden))\n",
    "# bnbias = torch.zeros((1, n_hidden))\n",
    "# bnmean_running = torch.zeros((1, n_hidden))\n",
    "# bnstd_running = torch.ones((1, n_hidden))\n",
    "\n",
    "# parameters = [C, W1, W2, b2, bngain, bnbias]\n",
    "# print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
    "# for p in parameters:\n",
    "#   p.requires_grad = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea866123-4ec5-4c37-8d9c-ab7476c929ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#7 BatchNormalisation\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "charEmbeddings  = torch.randn((vocab_size, n_embd),generator=g,requires_grad=True)\n",
    "weightLayer1 = torch.randn((n_embd * block_size, n_hidden), generator=g)*0.2\n",
    "biasLayer1 = torch.randn(n_hidden,generator=g)*0.01\n",
    "weightLayer2 = torch.randn((n_hidden, vocab_size),generator=g)*0.01\n",
    "biasLayer2 = torch.randn(vocab_size,generator=g)*0\n",
    "bngain = torch.ones((1,n_hidden))\n",
    "bnbias = torch.zeros((1,n_hidden))\n",
    "\n",
    "weightLayer2.requires_grad=True\n",
    "biasLayer2.requires_grad=True\n",
    "weightLayer1.requires_grad=True\n",
    "biasLayer1.requires_grad=True\n",
    "bngain.requires_grad=True\n",
    "bnbias.requires_grad=True\n",
    "\n",
    "#batchNormalization Additions\n",
    "\n",
    "\n",
    "\n",
    "parameters = [charEmbeddings, weightLayer1, weightLayer2, biasLayer1, biasLayer2, bngain, bnbias]\n",
    "\n",
    "max_steps=200000\n",
    "    batch_size=32\n",
    "    lossi = []\n",
    "    stepsi = []\n",
    "    noOfColAfterFlattening = block_size*n_embd\n",
    "    for i in range(max_steps):\n",
    "        noOfWindowsToBeUsedPerLearningStep = batch_size\n",
    "        randomSlidingWindowsUsed = torch.randint(0, inputContextTrain.shape[0], (noOfWindowsToBeUsedPerLearningStep,))\n",
    "        inputForLayer1 = charEmbeddings[inputContextTrain[randomSlidingWindowsUsed]]\n",
    "        inputForLayer1 = inputForLayer1.view(noOfWindowsToBeUsedPerLearningStep, noOfColAfterFlattening)\n",
    "        hpreActivation = inputForLayer1@weightLayer1+biasLayer1\n",
    "\n",
    "        #batchNormalization changes\n",
    "        bnmean  = hpreActivation.mean(0,keepdim=True)\n",
    "        bnstd = hpreActivation.std(0,keepdim=True)\n",
    "        hpreActivation = bngain*(hpreActivation-bnmean)/bnstd +bnbias\n",
    "        #Problem is this bnmean and bnstd are w.r.t current mini batch of size 32 and not for the whole dataset so for the validation step\n",
    "        #we dont have to have a global view for normalization i.e w.r.t to full data set so we need to keep to track of bnmean and bnstd for\n",
    "        #whole dataset and not just current mini batch\n",
    "\n",
    "    \n",
    "        \n",
    "                                 \n",
    "        h = torch.tanh(hpreActivation)\n",
    "        logits = h@weightLayer2+biasLayer2\n",
    "        loss = F.cross_entropy(logits,outputCharacterTrain[randomSlidingWindowsUsed])\n",
    "        for p in parameters:\n",
    "            p.grad = None \n",
    "        loss.backward()\n",
    "        lr = 0.1 if i<100000 else 0.01\n",
    "        for p in parameters:\n",
    "            p.data += -lr * p.grad \n",
    "        #track stats    \n",
    "        if i % 10000 == 0: # print every once in a while\n",
    "            print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}') #logging the loss in every 10,000 learning steps \n",
    "        lossi.append(loss.item())\n",
    "        stepsi.append(i)\n",
    "    \n",
    "       \n",
    "    plt.plot(stepsi,lossi)\n",
    "    print(\"loss on training data set\",loss.item())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3311e2-f536-4e29-bccd-92106c24d44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Solution for above problem:\n",
    "#Define bnMeanRunning, bnStdRunning\n",
    "\n",
    "bnStdRunning = torch.ones((1,n_hidden))  #they will not be optimized by backpropogation hence no graident\n",
    "bnMeanRunning = torch.zeros((1,n_hidden))\n",
    "\n",
    "parameters = [charEmbeddings, weightLayer1, weightLayer2, biasLayer1, biasLayer2, bngain, bnbias]\n",
    "\n",
    "max_steps=200000\n",
    "    batch_size=32\n",
    "    lossi = []\n",
    "    stepsi = []\n",
    "    noOfColAfterFlattening = block_size*n_embd\n",
    "    for i in range(max_steps):\n",
    "        noOfWindowsToBeUsedPerLearningStep = batch_size\n",
    "        randomSlidingWindowsUsed = torch.randint(0, inputContextTrain.shape[0], (noOfWindowsToBeUsedPerLearningStep,))\n",
    "        inputForLayer1 = charEmbeddings[inputContextTrain[randomSlidingWindowsUsed]]\n",
    "        inputForLayer1 = inputForLayer1.view(noOfWindowsToBeUsedPerLearningStep, noOfColAfterFlattening)\n",
    "        hpreActivation = inputForLayer1@weightLayer1+biasLayer1\n",
    "\n",
    "        #batchNormalization changes\n",
    "        bnmean  = hpreActivation.mean(0,keepdim=True)\n",
    "        bnstd = hpreActivation.std(0,keepdim=True)\n",
    "        hpreActivation = bngain*(hpreActivation-bnmean)/bnstd +bnbias\n",
    "        \n",
    "\n",
    "        learningDecay = 0.999\n",
    "        with torch.no_grad():\n",
    "           bnStdRunning =  learningDecay*bnStdRunning + (1-learningDecay)*bnstd\n",
    "           bnMeanRunning = learningDecay*bnMeanRunning + (1-learningDecay)*bnmean\n",
    "\n",
    "    \n",
    "        \n",
    "                                 \n",
    "        h = torch.tanh(hpreActivation)\n",
    "        logits = h@weightLayer2+biasLayer2\n",
    "        loss = F.cross_entropy(logits,outputCharacterTrain[randomSlidingWindowsUsed])\n",
    "        for p in parameters:\n",
    "            p.grad = None \n",
    "        loss.backward()\n",
    "        lr = 0.1 if i<100000 else 0.01\n",
    "        for p in parameters:\n",
    "            p.data += -lr * p.grad \n",
    "        #track stats    \n",
    "        if i % 10000 == 0: # print every once in a while\n",
    "            print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}') #logging the loss in every 10,000 learning steps \n",
    "        lossi.append(loss.item())\n",
    "        stepsi.append(i)\n",
    "    \n",
    "       \n",
    "    plt.plot(stepsi,lossi)\n",
    "    print(\"loss on training data set\",loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b59bcdc-c8a5-4321-9f80-ab7fa2b66617",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Validation step in batch normalization\n",
    "\n",
    "def validate(model_params, inputContextVal, outputCharacterVal):\n",
    "    \"\"\"Perform validation on a separate dataset.\"\"\"\n",
    "    total_loss = 0\n",
    "    num_samples = inputContextVal.shape[0]\n",
    "    \n",
    "    with torch.no_grad():  # Disable gradients during validation for efficiency\n",
    "        for i in range(0, num_samples, batch_size):\n",
    "            # Select a batch from validation data\n",
    "            batch_indices = torch.arange(i, min(i + batch_size, num_samples))\n",
    "            inputForLayer1 = charEmbeddings[inputContextVal[batch_indices]]\n",
    "            inputForLayer1 = inputForLayer1.view(len(batch_indices), noOfColAfterFlattening)\n",
    "\n",
    "            # Compute pre-activation\n",
    "            hpreActivation = inputForLayer1 @ weightLayer1 + biasLayer1\n",
    "\n",
    "            # --- Apply Batch Normalization using Running Mean & Variance ---\n",
    "            hpreActivation = (hpreActivation - bngainRunning) / (bnbiasRunning + 1e-5) * bngain + bnbias\n",
    "            \n",
    "            # Apply activation function\n",
    "            h = torch.tanh(hpreActivation)\n",
    "\n",
    "            # Compute logits\n",
    "            logits = h @ weightLayer2 + biasLayer2\n",
    "\n",
    "            # Compute validation loss (no backprop)\n",
    "            loss = F.cross_entropy(logits, outputCharacterVal[batch_indices])\n",
    "            total_loss += loss.item() * len(batch_indices)  # Sum up loss\n",
    "\n",
    "    # Compute average loss over the validation dataset\n",
    "    avg_loss = total_loss / num_samples\n",
    "    print(f\"Validation Loss: {avg_loss:.4f}\")\n",
    "    return avg_loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
